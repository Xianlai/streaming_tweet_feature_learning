{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m__archive\u001b[0m/                     \u001b[01;32mREADME.md\u001b[0m*\r\n",
      "\u001b[01;34m_docker\u001b[0m/                       Spark_overview.md\r\n",
      "\u001b[01;34mimgs\u001b[0m/                          StreamingPlot.py\r\n",
      "logs.txt                       tweet_feature_learning_SparkStreaming.ipynb\r\n",
      "pyspark_installation_guide.md  TweetsListener.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Tweets Feature Learning & Visualization\n",
    "\n",
    "### Xian Lai \n",
    "\n",
    "xlai4@fordham.edu   \n",
    "Dec.2017\n",
    "\n",
    "================================================================\n",
    "![](imgs/tweet_feature_learning.gif)\n",
    "![](imgs/logos.png)\n",
    "\n",
    "\n",
    "#### As the diagram showing above, this project implements a pipeline that learns predictive features from streaming tweets and visualizes the result in real-time.\n",
    "\n",
    "\n",
    "- **Receive streaming tweets on master machine**:    \n",
    "    Running the TweetsListener.py script in the background, a tweets stream with 3 tracks-\"NBA\", \"NFL\" and \"MBL\" are pulled from Tweepy API. Inside this stream, each tweet has a topic about one of those 3 tracks and is seperated by delimiter \"><\". \n",
    "\n",
    "\n",
    "- **Analysis tweets on distributed machines**:    \n",
    "    This stream is directed into Spark Streaming API through TCP connection and distributed onto cluster. Under the Spark Streaming API, the distrbuted stream is abstracted as a data type called DStream. A series of operations are then applied on this DStream in real time and transform it into other DStreams containing intermediate or final analysis results. \n",
    "    \n",
    "    1. preprocess each tweet into a label and a list of clean words which contains only numbers and alphabets.\n",
    "    2. count the frequencies of words in all tweets and take the top 5000 most frequent ones as features.\n",
    "    3. encode the tweets in last half minute into a structured dataset using features mentioned above.\n",
    "    4. calculate the conditional probability given label and the predictiveness of each feature word.\n",
    "    \n",
    "    \n",
    "- **Visualize results on master machine**:   \n",
    "    At last we select the tweets and features with higher predictiveness, collect their label, sum of predictiveness and 2 tsne features back onto the master machine and visualize them as a scatter plot. This visualization can be used as a informal validation of predictiveness defined above. If the scatter of different are well seperated, then the features selected by this predictiveness measure are valid.\n",
    "    \n",
    "    1. keep only 300 most predictive features and discard other non-predictive features.\n",
    "    2. calculate the sum of predictiveness of each word in tweet.\n",
    "    3. take 60 tweets with the highest sum of predictiveness under each label.\n",
    "    4. apply TSNE learning on these 300 data points to reduce dimentionality from 100 to 2 for visualization.\n",
    "\n",
    "\n",
    "#### The reasons using Spark Streaming for this project are:\n",
    "1. Spark streaming handles the in-coming stream in small batches. It is very flexible for either incremental or recomputational calculations(In our case, the word counts are updated on previous result and all others are recomputed from scratch every 5 seconds). \n",
    "2. Streaming computing and batch computing are well intergrated thus the algorithm is scalable. \n",
    "3. We can easily apply other existing RDD operations under Spark framework like machine learning and SQL querying on the DStream. \n",
    "4. Although there are many other tools can handle streaming data in distributed machines, Spark's core abstraction RDD, which caches data in memory, saves time for data I/O from external storage system and thus makes iterative calculation much more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io import _state; print(_state.uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"from bokeh import io; io._destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(`.${CLASS_NAME.split(' ')[0]}`);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.9.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.9.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.9.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.9.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.9.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"70eefeb6-d413-4b4e-b8f6-bf7a3ce8125e\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from re import sub\n",
    "from subprocess import Popen\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "import StreamingPlot as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global functions & variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeHeader():\n",
    "    \"\"\"Create a header containing starting time for each \n",
    "    round of analysis.\"\"\"\n",
    "    \n",
    "    global t_0\n",
    "    t_0 = datetime.now()\n",
    "    \n",
    "    return \"\\n------------------------- Time: \" + \\\n",
    "           str(t_0) + \" -------------------------\\n\"\n",
    "\n",
    "\n",
    "def minMaxScale(arr, bounds=(0, 1)):\n",
    "    \"\"\" Scale the given numeric sequence into given range.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    arr: sequence to be scaled\n",
    "    bounds: the lower and upper bounds as a tuple\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    scaled sequence as an 1-d numpy array\n",
    "    \"\"\"\n",
    "    if type(arr) is not np.ndarray: arr = np.array(arr)\n",
    "      \n",
    "    # calculate the range between 2 bounds\n",
    "    # scale the array into 0-1 bounds\n",
    "    # scale the array into given bounds\n",
    "    rng = bounds[1] - bounds[0]  \n",
    "    arr = (arr - min(arr))/(max(arr) - min(arr))\n",
    "    arr = (arr * rng) + bounds[0]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "# global variables in order:\n",
    "# the tags(labels) for tweets\n",
    "# the colors for each tag\n",
    "# run the tweets listener in the background\n",
    "# init a streaming plot object\n",
    "# init a empty plotting data source\n",
    "tags = ['nba', 'nfl', 'mlb']                                \n",
    "cols = ['red', 'blue', 'green']                             \n",
    "_    = Popen([\"python\", \"TweetsListener.py\"])               \n",
    "plt  = sp.StreamingPlot(interval=5, width=900, height=900)  \n",
    "pds  = {'x':[], 'y':[], 'color':[], 'tags':[], 'size':[], 'alpha':[]}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive tweets:\n",
    "\n",
    "After we run the listerner.py file as a background app, the incoming tweets will be available at IP:172.17.0.2, port:5555.   \n",
    "\\*Here the IP address is the docker container's IP, not local machine's.\n",
    "\n",
    "On the Spark end, the stream is directed into a data abstraction called DStream which is a sequence of RDD's. Given interval time, each RDD holds tweets came in through socket during last interval and are distributed on different machines in cluster. And when we apply operations on the DStream, we are actually manipulating the RDD underlying.\n",
    "\n",
    "Because the in coming stream is messy, there are situations when multiple tweets in one line or one tweet in multiple line. When we receive tweets, we first organize them by delimiter so each row in RDD will be a single tweet.\n",
    "\n",
    "    example raw tweets:\n",
    "\tHere's every Tom Brady Postseason TD! #tbt #NFLPlayoffs https://t.co/2CIHBpz2OW...\n",
    "\tRT @ChargersRHenne: This guy seems like a class act.  I will root for him\n",
    "\tRT @NBA: Kyrie ready! #Celtics #NBALondon https://t.co/KgZVsREGUK...\n",
    "\tRT @NBA: The Second @NBAAllStar Voting Returns! https://t.co/urTwnGQNKl...\n",
    "\t...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark configuration using local mode with all CPU's\n",
    "# create a spark context with this configuration\n",
    "# create a streaming context and set the interval to 5 second.\n",
    "# set log level for spark context\n",
    "# set checkpoint directory for spark streaming context\n",
    "conf = SparkConf().setMaster(\"local[*]\")\n",
    "sc   = SparkContext(conf=conf)\n",
    "ssc  = StreamingContext(sc, batchDuration=5)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# create a DStream by listening to socket\n",
    "# For each RDD in stream:\n",
    "# - split the tweets by delimiter \"><\"\n",
    "# - flat map all the rows contains list\n",
    "# get n rows: (tweet)\n",
    "rawTweets = ssc.socketTextStream('172.17.0.2',5555)\\\n",
    "    .map(lambda x: x.split(\"><\"))\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    \n",
    "def logRawTweets(rdd):\n",
    "    \"\"\" Save the first 5 raw tweets in external text file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(timeHeader())\n",
    "        file.write(\"rawTweet:\\n\\t{} raw tweets received from port:\\n\".format(rdd.count()))\n",
    "        for row in rdd.take(5): \n",
    "            file.write('\\t' + str(row[:80]) + '...\\n')\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        # calculate how much time spent on this step\n",
    "        t_1 = datetime.now()\n",
    "        file.write(\"time for receiving raw tweets: \"+str(t_1 - t_0)+\"\\n\")\n",
    "\n",
    "rawTweets.foreachRDD(logRawTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze tweets:\n",
    "Then we will apply function on this rawTweets DStream and transform it into new streams.\n",
    "\n",
    "![](imgs/DStreams.png)\n",
    "\n",
    "\n",
    "### 1. preprocessing\n",
    "In the raw stream, each tweet is a line of string consist of alphabets, numbers, punctuation marks and possibly symbols. In this preprocessing step, we will split the string into a list of clean words, remove symbols like punctuation marks and emojis and assign a tag for each tweet.\n",
    "\n",
    "    example cleaned tweets after preprocessing:\n",
    "\ttag:1, words:['rt', 'chargersrhenne', 'this', 'guy', ...],\n",
    "\ttag:0, words:['rt', 'debruynekev', 'amp', 'ilkayguendogan', ...],\n",
    "\ttag:0, words:['rt', 'commissioner', 'adam', 'silver', ...],\n",
    "\ttag:0, words:['rt', 'spurs', 'all', 'star', ...],\n",
    "\ttag:0, words:['nbaallstar', 'karlanthony', 'towns', 'nbavote', ...],\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignTag(tweet):\n",
    "    \"\"\" Assign a tag for given tweet and remove the tag word from this tweet. \n",
    "    If a tweet has none of the tag words, it is discarded. If a tweet has\n",
    "    more than one tag words, it is assign a tag in order: NBA, NFL, MLB.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag in tweet:\n",
    "            return (i, [word for word in tweet if word != tag])\n",
    "\n",
    "        \n",
    "# For each RDD in stream:\n",
    "# - split the words\n",
    "# - strip the symbols and keep only numbers and alphabets\n",
    "# - lower case the words\n",
    "# - remove empty strings\n",
    "# - assign tags for each tweet and remove tag words\n",
    "# get n rows: (tag, [word_0, word_1, ..., word_m])\n",
    "cleanTweets = rawTweets\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .map(lambda xs: [sub('[^A-Za-z0-9]+', '', x) for x in xs])\\\n",
    "    .map(lambda xs: [x.lower() for x in xs])\\\n",
    "    .map(lambda xs: [x for x in xs if x != ''])\\\n",
    "    .map(assignTag)\\\n",
    "    .filter(lambda x: x != None)\n",
    "\n",
    "    \n",
    "def logCleanTweets(rdd):\n",
    "    \"\"\" Save first 5 cleaned tweets and time spent in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\ncleanTweet:\\n\\tcleaned tweets after preprocessing:\\n\")\n",
    "        for row in rdd.take(5):\n",
    "            w = str(row[1][:4]).rstrip(']') + ', ...],\\n'\n",
    "            file.write('\\ttag:'+str(row[0])+\", words:\" + w)\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for cleaning tweets: \"+str(datetime.now()-t_1)+\"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "cleanTweets.foreachRDD(logCleanTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature extraction\n",
    "Next, we count the frequency of each word ever appeared in the streaming tweets and keep updating the counts when new data comes in.\n",
    "\n",
    "    wordCount:\n",
    "\t('rt', 196)\n",
    "\t('the', 174)\n",
    "\t('in', 85)\n",
    "\t('for', 62)\n",
    "\t('to', 59)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWordCount(newValues, oldCount):\n",
    "    \"\"\" Update the word count with new data.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    newValues: (word, 1) tuples in this round\n",
    "    oldCount: (word, count) tuples upto last round.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    (word, count) tuples upto this round.\n",
    "    \"\"\"\n",
    "    return sum(newValues) + (oldCount or 0)\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - remove the tag\n",
    "# - flatten the list of words so each row has one word\n",
    "# - add 1 count to each word\n",
    "# - use the new word count to update old word count incrementally\n",
    "# - sort the word and count tuple by the count in descending order\n",
    "# get n rows: (word, count)\n",
    "wordCount = cleanTweets\\\n",
    "    .map(lambda x: x[1])\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .updateStateByKey(updateWordCount)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the word counts, we take the 5000 most frequent words as features for tweets dataset. Each feature value in dataset is either True or False indicating whether corresponding tweet contains this feature word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each RDD in stream:\n",
    "# - zip rows with index\n",
    "# - filter the first 5000 rows\n",
    "# - discard the index\n",
    "# get 5000 rows: (word, count)\n",
    "features = wordCount\\\n",
    "    .transform(lambda rdd: rdd.zipWithIndex())\\\n",
    "    .filter(lambda x: x[1] < 5000)\\\n",
    "    .map(lambda x: x[0])\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - take only the word\n",
    "# get 5000 rows: (word)\n",
    "words = features.map(lambda x: x[0])\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - take only the count\n",
    "# get 5000 rows: (count)\n",
    "counts = features.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "def logFeatureWords(rdd):\n",
    "    \"\"\" Save first 5 word count and time spent in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nfeatures:\\n\\twords used as features and their counts:\\n\")\n",
    "        for row in rdd.take(num=5): file.write('\\t' + str(row) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for generating and extracting features:\" \\\n",
    "                   + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "features.foreachRDD(logFeatureWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. encode dataset\n",
    "With the feature words avaliable, we can transform the tweets into a structured dataset for further analysis. Because the amount of tweets in 5 seconds is limited. Here we use window slicing to retrieve and combine tweets in a longer range of time. To be specific, we will fetch all tweets in last 15 seconds and combine them as one RDD every 5 second.\n",
    "\n",
    "\texample encoded dataset:\n",
    "\ttag: 0, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 1, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 2, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 0, features: [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...],\n",
    "\ttag: 1, features: [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, ...],\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDataSet(rdd_a, rdd_b):\n",
    "    \"\"\" Encode given tweets by given feature words.\n",
    "    If a feature word appears in this tweet, the corresponding\n",
    "    feature is 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    words_  = rdd_b.collect()\n",
    "    rdd_new = rdd_a.mapValues(lambda x: \\\n",
    "        ([int(word in x) for word in words_], 1))\n",
    "    return rdd_new\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - get tweets in 15s window every 5s\n",
    "# - encoded the tweets using features and append value 1\n",
    "#   onto each tweet to count the number of tweets later.\n",
    "# get rows: (tag, ([1, 0, ..., 1], 1))\n",
    "dataset = cleanTweets\\\n",
    "    .window(15, 5)\\\n",
    "    .transformWith(encodeDataSet, words)\n",
    "\n",
    "def logDataset(rdd):\n",
    "    \"\"\" Save first 10 rows in encoded dataset and time spent \n",
    "    in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nDataset:\\n\\tencoded dataset:\\n\")\n",
    "        for row in rdd.take(10):\n",
    "            t = str(row[0])\n",
    "            f = str(row[1][0][:10]).rstrip(']') + ', ...],\\n'\n",
    "            file.write(\"\\ttag: \" + t + \", features: \" + f)\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for encoding dataset:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "dataset.foreachRDD(logDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. calculate \"predictiveness\" for features\n",
    "\n",
    "**p(feature|tag)**:   \n",
    "Since all the feature values are either 0 or 1, we can easily get the probability of one feature conditioned on any tag by summing up the values of this column under corresponding tag and divide it by the total count of tweets under this tag.\n",
    "\n",
    "**predictiveness**:  \n",
    "Then we calculate the predictiveness of each feature from this dataset. The predictiveness of one feature quantifies how well can a feature discriminate the label of a tweet from other 2 labels.\n",
    "\n",
    "    1. If we have only 2 labels, it can be defined by the bigger ratio of conditional probabilities of this feature given tags.\n",
    "    \n",
    "$$pdtn(label_1, label_2) = arg max\\left( \\frac{p(word|label_1)}{p(word|label_2)}, \\frac{p(word|label_2)}{p(word|label_1)}\\right)$$\n",
    "\n",
    "\\*Note that this measure is symmetric. In other words, how much a feature can distinguish label 1 from label 2 is the same as how much it can distinguish label 2 from label 1.\n",
    "    \n",
    "    2. When we have more than 2 labels, we can take the average of the predictivenesses of all label combinations.\n",
    "    \n",
    "$$pdtn(label_1, label_2, label_3) = \\frac{pdtn(label_1, label_2) + pdtn(label_1, label_3) + pdtn(label_2, label_3)}{3}$$\n",
    "\n",
    "    3. At last this predictiveness of feature word should be weighted by the count of this word. The more frequent this word appears, the more reliable this predictiveness is.\n",
    "\n",
    "$$pdtn = pdtn \\times count$$\n",
    "\n",
    "    most predictive words:\n",
    "\tword        : (cp0, cp1, cp2, pdtn)\n",
    "\tdon         : (0.15934065934065933, 0.0078125, 0.05555555555555555, 283.4985066666667)\n",
    "\tallstar     : (0.14835164835164835, 0.0078125, 0.05555555555555555, 249.3439)\n",
    "\tthe         : (0.46153846153846156, 0.4765625, 0.3333333333333333, 244.9167533333333)\n",
    "\talabama     : (0.005494505494505495, 0.140625, 0.05555555555555555, 216.67129)\n",
    "\trt          : (0.5879120879120879, 0.625, 0.5555555555555556, 211.0108)\n",
    "\tfitzpatrick : (0.005494505494505495, 0.1328125, 0.05555555555555555, 195.5925333333333)\n",
    "\tvoting      : (0.12637362637362637, 0.0078125, 0.05555555555555555, 187.45217333333335)\n",
    "\tminkah      : (0.005494505494505495, 0.125, 0.05555555555555555, 175.55554999999998)\n",
    "\tdraft       : (0.016483516483516484, 0.171875, 0.1111111111111111, 149.7176)\n",
    "\tknow        : (0.16483516483516483, 0.0234375, 0.05555555555555555, 131.95061333333334)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementWiseAdd(list_1, list_2):\n",
    "    \"\"\" Add up 2 lists element-wisely\"\"\"\n",
    "    return [a + b for a, b in zip(list_1, list_2)]\n",
    "\n",
    "def calcPDTN(cps):\n",
    "    \"\"\" takes in the conditional probablities of one feature \n",
    "    and calculate the predictiveness of this feature.\n",
    "    \"\"\"\n",
    "    cp0, cp1, cp2 = cps\n",
    "    f    = lambda a, b: round(max(a/b, b/a), 5)\n",
    "    pdtn = (f(cp0, cp1) + f(cp0, cp2) + f(cp1, cp2))/3\n",
    "\n",
    "    return (cp0, cp1, cp2, pdtn)\n",
    "\n",
    "\n",
    "def weightByCount(rdd_a, rdd_b):\n",
    "    \"\"\" Weight the predictiveness of features by their counts.\"\"\"\n",
    "    # collect the counts as a list\n",
    "    # for each row in rdd:\n",
    "    # weight the pdtn by corresponding count\n",
    "    # get 5000 rows: (index, (cp0_m, cp1_m, cp2_m, weightedPDTN_m))\n",
    "    cnts    = rdd_b.collect()\n",
    "    rdd_new = rdd_a.map(lambda x: (x[0], (x[1][0], x[1][1], \\\n",
    "        x[1][2], x[1][3]*cnts[x[0]])))\n",
    "    \n",
    "    return rdd_new\n",
    "        \n",
    "\n",
    "# For each RDD in stream:\n",
    "# - reduce the RDD by key so that each RDD has only two rows,\n",
    "#   each row has 3 elements: the tag, counts of appearances \n",
    "#   of each feature word in the tweets and the count of tweets\n",
    "# get 3 rows corresponding to 3 tags: \n",
    "#   (tag, ([featCnt_0, featCnt_1, ..., featCnt_4999], tweetCnt))\n",
    "# - divide the count of each feature word by the count of tweets\n",
    "#   to get the conditional probability. We add 1 to the count \n",
    "#   to avoid zero divide error.\n",
    "# get 3 rows: (tag, [cp_0, cp_1, ..., cp_4999])\n",
    "# - for each row, combine each cp with tag\n",
    "# get 3 rows: ((0, cp_0), (1, cp_1), ..., (4999, cp_4999))\n",
    "# - flatmap these 3 rows\n",
    "# get 15000 rows: (m, cp_m)\n",
    "# - reduce by key to combine cps for different tags\n",
    "# get 5000 rows: (index, (cp0_m, cp1_m, cp2_m))\n",
    "# - for each row, calculate predictiveness using conditional prob\n",
    "# get 5000 rows: (index, (cp0_m, cp1_m, cp2_m, pdtn_m))\n",
    "# - weight the pdtn by the count of the words\n",
    "# get 5000 rows: (index, (cp0_m, cp1_m, cp2_m, weightedPDTN_m))\n",
    "featureStats = dataset\\\n",
    "    .reduceByKey(lambda a, b: (elementWiseAdd(a[0], b[0]), a[1]+b[1]))\\\n",
    "    .mapValues(lambda x: [(count+1)/x[1] for count in x[0]])\\\n",
    "    .map(lambda x: [(i, cp) for i, cp in enumerate(x[1])])\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    .mapValues(lambda x: [x])\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .map(lambda x: (x[0], calcPDTN(x[1])))\\\n",
    "    .transformWith(weightByCount, counts)\n",
    "    \n",
    "def zipWithWords(rdd_a, rdd_b):\n",
    "    \"\"\" Replace the feature index with word.\n",
    "    \"\"\"\n",
    "    words_ = rdd_b.collect()\n",
    "    return rdd_a.map(lambda x: (words_[x[0]], x[1]))\n",
    "    \n",
    "def logFeatureStats(rdd):\n",
    "    \"\"\" Save top 10 most predictive words and their stats to log file.\"\"\"\n",
    "    global t_1\n",
    "    rdd_new = rdd.sortBy(lambda x: -x[1][3])\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nmost predictive words:\\n\\tword : (cp0, cp1, cp2, pdtn)\\n\")\n",
    "        for row in rdd_new.take(10): \n",
    "            file.write(\"\\t\" + row[0] + \" : \" + str(row[1]) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for calculating predictiveness:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "featureStats.transformWith(zipWithWords, words)\\\n",
    "    .foreachRDD(logFeatureStats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictive features\n",
    "\n",
    "To validate whether this feature predictiveness makes sense, we take top 300 most predictive features out of 5000. Under each label, 60 tweets are selected for a 2-d scatter plot. \n",
    "\n",
    "In the plotting, each circle represents a tweet. The color is identified by the tweet label-NBA tweets will be red, NFL tweets will be blue and MLB ones will be green. The size and alpha of circle is identified by the sum of predictivenesses of words in each tweet. And the x, y values are 2 dimensions learned from those 300 features of tweets using t-distributed stochastic neighbor embedding.\n",
    "\n",
    "\\*All the numeric attributes are standardized using min-max scaler to produce more readable result.\n",
    "\n",
    "    plotting data source:\n",
    "\tTotal number of tweets:138\n",
    "\tNumber of NBA tweets; NFL tweets; MLB tweets : [60, 60, 18]\n",
    "\tx     : [ 0.17929186  0.18399966  0.63295108  0.17661807...]\n",
    "\ty     : [ 0.62392987  0.66881742  0.69876889  0.36454208...]\n",
    "\tcolor : ['red', 'red', 'red', 'red'...]\n",
    "\ttags  : ['nba', 'nba', 'nba', 'nba'...]\n",
    "\tsize  : [0.0042378419396584864, 0.0042378419396584864, 0.0042378419396584864, 0.0042378419396584864...]\n",
    "\talpha : [ 0.42378419  0.42378419  0.42378419  0.42378419...]\n",
    "\n",
    "If the tweets are distinguishable and the feature predictiveness really captures the information inside tweets, the circles in plotting will be well seperated.\n",
    "\n",
    "### 1. Select highly predictive features and tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchFeatsTweets(rdd_a, rdd_b):\n",
    "    \"\"\" Given predictiveness of each feature word, fetch 300 most\n",
    "    predictive features and 180 most predictable tweets under 3 tags\n",
    "    from encoded dataset.\n",
    "    \"\"\"\n",
    "    # For each RDD:\n",
    "    # - take the index and pdtn of features\n",
    "    # get 5000 rows: (index, pdtn_i)\n",
    "    # - sort by pdtn\n",
    "    # get 5000 rows: (index, pdtn_i)\n",
    "    # - discard the pdtn and take the first 300 indices\n",
    "    featIndex = rdd_b\\\n",
    "        .map(lambda x: (x[0], x[1][3]))\\\n",
    "        .sortBy(lambda x: -x[1])\\\n",
    "        .map(lambda x: x[0])\\\n",
    "        .take(num=300)\n",
    "        \n",
    "    # collect the predictiveness of each feature word as a list\n",
    "    pdtns = rdd_b.map(lambda x: x[1][3]).collect()\n",
    "    sumPDTN = lambda a, b: sum([i*k for i, k in zip(a, b)])\n",
    "    \n",
    "    # for each row in rdd:\n",
    "    # has n rows: (tag, ([feat_0, ..., feat_4999], 1))\n",
    "    # - keeps only high predictive features\n",
    "    # get n rows: (tag, ([feat_0, ..., feat_299], 1))\n",
    "    # - calculate the sum pdtn\n",
    "    # get n rows: (tag, [feat_0, ..., feat_299], sumPDTN)\n",
    "    # - sort by sumPDTN\n",
    "    # get n rows: (tag, [feat_0, ..., feat_299], sumPDTN)\n",
    "    # - filter the first 60 rows:\n",
    "    # get 100 rows: (tag, [feat_0, ..., feat_299], sumPDTN)\n",
    "    rdd_new = rdd_a\\\n",
    "        .map(lambda x: (x[0], [x[1][0][i] for i in featIndex]))\\\n",
    "        .map(lambda x: (x[0], x[1], sumPDTN(x[1], pdtns)))\\\n",
    "        .sortBy(lambda x: -x[2])\\\n",
    "        .zipWithIndex()\\\n",
    "        .filter(lambda x: x[1] < 60)\\\n",
    "        .map(lambda x: x[0])\\\n",
    "    \n",
    "    return rdd_new\n",
    "\n",
    "# for each tag, filter dataset DStream with corresponding tag\n",
    "# fetch high predictive features and predictable tweets.\n",
    "ds_0 = dataset.filter(lambda x: x[0] == 0)\\\n",
    "    .transformWith(fetchFeatsTweets, featureStats)\n",
    "ds_1 = dataset.filter(lambda x: x[0] == 1)\\\n",
    "    .transformWith(fetchFeatsTweets, featureStats)\n",
    "ds_2 = dataset.filter(lambda x: x[0] == 2)\\\n",
    "    .transformWith(fetchFeatsTweets, featureStats)\n",
    "      \n",
    "# concatenate 3 DStreams together\n",
    "# get 300 rows: (tag, [feat_0, ..., feat_99], sumPDTN)\n",
    "selectedTweets = ds_0.union(ds_1).union(ds_2)\n",
    "\n",
    "\n",
    "def logSelectedTweets(rdd):\n",
    "    \"\"\" Save first 5 selected tweets and features to log file.\"\"\"\n",
    "    global t_1\n",
    "    cnt = rdd.count()\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nselected {} tweets and 100 features for plotting:\\n\".format(cnt))\n",
    "        for row in rdd.take(5): \n",
    "            feats = str(row[1][:20]).rstrip(']') + \", ...],\"\n",
    "            file.write(\"\\t\" + str(row[0]) + \", \" + feats + str(row[2]) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for filtering tweets and features:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "selectedTweets.foreachRDD(logSelectedTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. collect plotting data source and dimensionality reduction\n",
    "\n",
    "Since the dataset we have are sparse and have binary values, here we use non-linear method t-SNE to learn the 2-d manifold the dataset lies on to obtain x values and y values for our each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectPDS(rdd):\n",
    "    \"\"\" collect the tag, feature values and predictiveness sum \n",
    "    and fit-transform the features with TSNE.\n",
    "    \"\"\"\n",
    "    global tsne_0, tsne_1, color, label, alpha, t_1\n",
    "    \n",
    "    # transform tags indices to colors, and tags\n",
    "    # collect data to master machine\n",
    "    pData = rdd.map(lambda x: (cols[x[0]], tags[x[0]], x[2], x[1]))\n",
    "    pData = pData.collect()\n",
    "    \n",
    "    # reduce the dimensionality to 2 using t-SNE\n",
    "    # scale the learned features to range [0, 1]\n",
    "    array  = np.array([x[3] for x in pData])\n",
    "    if len(array.shape) == 1: \n",
    "        array = array.reshape(1, -1)\n",
    "    embed  = TSNE(n_components=2).fit_transform(array)\n",
    "    tsne_0 = minMaxScale(embed[:, 0])\n",
    "    tsne_1 = minMaxScale(embed[:, 1])\n",
    "    \n",
    "    color = [x[0] for x in pData]\n",
    "    label = [x[1] for x in pData]\n",
    "    alpha = [x[2] for x in pData]\n",
    "    \n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\ntime for TSNE learning:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "    t_1 = datetime.now()\n",
    "    \n",
    "selectedTweets.foreachRDD(collectPDS)\n",
    "\n",
    "\n",
    "def printPDS(pds):\n",
    "    \"\"\" Save plotting data source to log file.\"\"\"\n",
    "    global t_1\n",
    "    lens = [len([x for x in pds['color'] if x == i]) for i in cols]\n",
    "    \n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nplotting data source:\\n\")\n",
    "        file.write(\"\\tTotal number of tweets:\" + str(len(pds['x'])) + \"\\n\")\n",
    "        file.write(\"\\tNumber of NBA tweets; NFL tweets; MLB tweets : \" + str(lens) + \"\\n\")\n",
    "        for item in pds.items(): \n",
    "            file.write(\"\\t\" + item[0] + \" : \" + str(item[1][:4]).rstrip(']') + \"...]\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for collecting plotting data source:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "        file.write(\"\\nTiming for this round: {}\\n\".format(t_1 - t_0))\n",
    "\n",
    "        \n",
    "def updatePDS(rdd):\n",
    "    \"\"\" update the dictionary containing data for plotting.\n",
    "    \"\"\"\n",
    "    global alpha\n",
    "    alpha = minMaxScale(alpha, (0.2, 0.8))\n",
    "    sizes = [i/100 for i in alpha]\n",
    "    \n",
    "    global pds\n",
    "    pds['x']     = tsne_0\n",
    "    pds['y']     = tsne_1\n",
    "    pds['color'] = color\n",
    "    pds['tags']  = label\n",
    "    pds['alpha'] = alpha\n",
    "    pds['size']  = sizes\n",
    "    \n",
    "    printPDS(pds)\n",
    "     \n",
    "selectedTweets.foreachRDD(updatePDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computation pipeline set, we start the Spark Streaming context and show the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"5dc5012d-757f-4c99-991f-d6fd253c39a3\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"bd9a2748-173c-4482-9e09-bf3ae2999f9a\":{\"roots\":{\"references\":[{\"attributes\":{\"callback\":null},\"id\":\"bb688d13-73e8-451f-8090-5b94e519e671\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"90b15c44-7657-4ae5-8c9e-d4ffd98ba1ff\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"2addfb9e-0b87-462a-8605-6fe619d616d0\",\"type\":\"LinearScale\"},{\"attributes\":{\"grid_line_color\":{\"value\":null},\"minor_grid_line_color\":{\"value\":\"#aaaaaa\"},\"minor_grid_line_dash\":[2,4],\"plot\":{\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6472bf35-e222-4f2c-945e-284662b3f951\",\"type\":\"BasicTicker\"}},\"id\":\"edd45657-5931-4f0b-a089-39a8bf0aa6cf\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"57e687c3-9a24-4fa7-b866-cb693aaf2a82\",\"type\":\"PanTool\"},{\"attributes\":{\"data_source\":{\"id\":\"2fcee919-6b6d-4e79-a964-be6368a41a77\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1fa83baa-403d-40aa-8f37-3f82b0d51758\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"18b2ba93-118e-44aa-95da-cd6fa4934c22\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"8ed5c08f-6acb-4941-b291-9597ffa3a5b0\",\"type\":\"CDSView\"}},\"id\":\"d3d15bb3-4419-41f7-bbeb-7ca1a0c2df17\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"axis_label\":\"tsne_feature_1\",\"formatter\":{\"id\":\"6bc3855e-de7f-41be-8b33-def961656436\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"90b15c44-7657-4ae5-8c9e-d4ffd98ba1ff\",\"type\":\"BasicTicker\"}},\"id\":\"44261c3b-b74b-450b-becd-144772ea5179\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"radius\":{\"field\":\"size\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"18b2ba93-118e-44aa-95da-cd6fa4934c22\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\",\"color\",\"alpha\",\"tags\",\"size\"],\"data\":{\"alpha\":[],\"color\":[],\"size\":[],\"tags\":[],\"x\":[],\"y\":[]}},\"id\":\"2fcee919-6b6d-4e79-a964-be6368a41a77\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"deb819f8-7fbf-46bd-9ff1-216fdc3b4e26\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"4b1d5efb-d0e4-44eb-ad7e-8b01c4bcd45e\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"571cfa4b-cf50-4f01-adf4-d32756de394a\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"dd995f99-fc1f-4748-b660-1a22c53371d3\",\"type\":\"SaveTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"57e687c3-9a24-4fa7-b866-cb693aaf2a82\",\"type\":\"PanTool\"},{\"id\":\"4b1d5efb-d0e4-44eb-ad7e-8b01c4bcd45e\",\"type\":\"WheelZoomTool\"},{\"id\":\"571cfa4b-cf50-4f01-adf4-d32756de394a\",\"type\":\"ResetTool\"},{\"id\":\"dd995f99-fc1f-4748-b660-1a22c53371d3\",\"type\":\"SaveTool\"}]},\"id\":\"d3e1cc95-2715-4509-9e2a-076303671b7e\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"58767c73-c12e-49e8-8e80-a6e309a6d2a3\",\"type\":\"LinearScale\"},{\"attributes\":{\"dimension\":1,\"grid_line_color\":{\"value\":null},\"grid_line_dash\":[2,4],\"minor_grid_line_color\":{\"value\":\"#aaaaaa\"},\"minor_grid_line_dash\":[2,4],\"plot\":{\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"90b15c44-7657-4ae5-8c9e-d4ffd98ba1ff\",\"type\":\"BasicTicker\"}},\"id\":\"9619d71a-efc6-4572-b22c-2abb2e76ee55\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"6bc3855e-de7f-41be-8b33-def961656436\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"field\":\"alpha\"},\"fill_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0},\"line_color\":{\"value\":\"#1f77b4\"},\"radius\":{\"field\":\"size\",\"units\":\"data\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1fa83baa-403d-40aa-8f37-3f82b0d51758\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null},\"id\":\"eaab12f2-0a97-4086-9106-cc8900033140\",\"type\":\"DataRange1d\"},{\"attributes\":{\"background_fill_color\":{\"value\":\"#efefef\"},\"below\":[{\"id\":\"f1db7410-0f3d-45b6-8c60-8cebc5ed251e\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"44261c3b-b74b-450b-becd-144772ea5179\",\"type\":\"LinearAxis\"}],\"plot_height\":900,\"plot_width\":900,\"renderers\":[{\"id\":\"f1db7410-0f3d-45b6-8c60-8cebc5ed251e\",\"type\":\"LinearAxis\"},{\"id\":\"edd45657-5931-4f0b-a089-39a8bf0aa6cf\",\"type\":\"Grid\"},{\"id\":\"44261c3b-b74b-450b-becd-144772ea5179\",\"type\":\"LinearAxis\"},{\"id\":\"9619d71a-efc6-4572-b22c-2abb2e76ee55\",\"type\":\"Grid\"},{\"id\":\"ac8f8197-d4a2-4f7c-b2c4-58fdf0efc948\",\"type\":\"Legend\"},{\"id\":\"d3d15bb3-4419-41f7-bbeb-7ca1a0c2df17\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"e6ab7322-3e55-4d0d-8442-a71be78e5db2\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"d3e1cc95-2715-4509-9e2a-076303671b7e\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"eaab12f2-0a97-4086-9106-cc8900033140\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"2addfb9e-0b87-462a-8605-6fe619d616d0\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"bb688d13-73e8-451f-8090-5b94e519e671\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"58767c73-c12e-49e8-8e80-a6e309a6d2a3\",\"type\":\"LinearScale\"}},\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"source\":{\"id\":\"2fcee919-6b6d-4e79-a964-be6368a41a77\",\"type\":\"ColumnDataSource\"}},\"id\":\"8ed5c08f-6acb-4941-b291-9597ffa3a5b0\",\"type\":\"CDSView\"},{\"attributes\":{\"plot\":null,\"text\":\"Streaming text analysis\"},\"id\":\"e6ab7322-3e55-4d0d-8442-a71be78e5db2\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"tsne_feature_0\",\"formatter\":{\"id\":\"deb819f8-7fbf-46bd-9ff1-216fdc3b4e26\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6472bf35-e222-4f2c-945e-284662b3f951\",\"type\":\"BasicTicker\"}},\"id\":\"f1db7410-0f3d-45b6-8c60-8cebc5ed251e\",\"type\":\"LinearAxis\"},{\"attributes\":{\"label\":{\"field\":\"tags\"},\"renderers\":[{\"id\":\"d3d15bb3-4419-41f7-bbeb-7ca1a0c2df17\",\"type\":\"GlyphRenderer\"}]},\"id\":\"dc1d90a0-078c-4d32-ad45-4ef0a1ed5012\",\"type\":\"LegendItem\"},{\"attributes\":{\"items\":[{\"id\":\"dc1d90a0-078c-4d32-ad45-4ef0a1ed5012\",\"type\":\"LegendItem\"}],\"plot\":{\"id\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"ac8f8197-d4a2-4f7c-b2c4-58fdf0efc948\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"6472bf35-e222-4f2c-945e-284662b3f951\",\"type\":\"BasicTicker\"}],\"root_ids\":[\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.9\"}};\n",
       "    var render_items = [{\"docid\":\"bd9a2748-173c-4482-9e09-bf3ae2999f9a\",\"elementid\":\"5dc5012d-757f-4c99-991f-d6fd253c39a3\",\"modelid\":\"1967a149-4ec6-4c38-a94b-2d4cd229f6bb\",\"notebook_comms_target\":\"f1cb9a79-830c-4901-8b0d-ee12434cb22e\"}];\n",
       "\n",
       "    root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "  }\n",
       "\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to embed document because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1967a149-4ec6-4c38-a94b-2d4cd229f6bb"
      }
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b62c9a4b65be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/StreamingPlot.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mpush_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "plt.start(pds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
