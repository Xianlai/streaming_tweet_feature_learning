{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Tweets Feature Learning & Visualization\n",
    "\n",
    "### Xian Lai \n",
    "\n",
    "xlai4@fordham.edu   \n",
    "Dec.2017\n",
    "\n",
    "================================================================\n",
    "![](imgs/tweet_feature_learning.gif)\n",
    "\n",
    "## Abstract\n",
    "As the role of big data analysis playing in ecommerce becoming increasingly important, more and more streaming computation systems like Storm, Hadoop are developed trying to satisfy requirements of both time and space efficiency as well as accuracy. \n",
    "\n",
    "Among them, Spark Streaming is a great tool for mini-batch real-time streaming analysis running on distributed computing system. Under the Spark Streaming API, the data stream is distributed on many workers and abstracted as a data type called DStream. DStream is essentially a sequence of Resilient Distributed Datasets(RDDs). A series of operations can then applied on this sequence of RDD's in real-time and transform it into other DStreams containing intermediate or final analysis results.\n",
    "\n",
    "In this project, a life tweets streaming are pulled from Tweeter API. 300 most predictive features words for hashtag classification are learned from this stream using Spark Streaming library in real-time. To validate the learning process, these 300 features are are reduced to a 2-d coordinate system. New tweets are plot on these 2 dimensions as scatter. As more and more tweets  learned by the system, the tweets with same hashtag gradualy aggregate together on this 2-d coordinates which means they are easily separable based on this features.\n",
    "\n",
    "#### The reasons using Spark Streaming for this project are:\n",
    "1. Spark streaming handles the in-coming stream in small batches. It is very flexible for either incremental or recomputational calculations(In our case, the counts and predictiveness of feature words are updated incrementally on previous result and all others are recomputed from scratch every 5 seconds). \n",
    "2. Streaming computing and distributed computing are well intergrated thus the algorithm is scalable. \n",
    "3. We can easily apply other existing RDD operations under Spark framework like machine learning functions and SQL queries on the DStream. \n",
    "4. Although there are many other tools can handle streaming data in distributed machines, Spark's core abstraction RDD, which caches data in memory, saves time for data I/O from external storage system and thus makes iterative calculation much more efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from subprocess import Popen\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.serializers import AutoBatchedSerializer, BatchedSerializer, MarshalSerializer\n",
    "import numpy as np\n",
    "import StreamingPlot as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time learning pipeline\n",
    "\n",
    "![](imgs/logos.png)\n",
    "\n",
    "\n",
    "## Global functions & variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeHeader():\n",
    "    \"\"\"Create a header containing starting time for each \n",
    "    round of analysis.\"\"\"\n",
    "    \n",
    "    global t_0\n",
    "    t_0 = datetime.now()\n",
    "    \n",
    "    return \"\\n------------------------- Time: \" + \\\n",
    "           str(t_0) + \" -------------------------\\n\"\n",
    "\n",
    "\n",
    "def minMaxScale(arr, bounds=(0, 1)):\n",
    "    \"\"\" Scale the given numeric sequence into given range.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    arr: sequence to be scaled\n",
    "    bounds: the lower and upper bounds as a tuple\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    scaled sequence as an 1-d numpy array\n",
    "    \"\"\"\n",
    "    if type(arr) is not np.ndarray: arr = np.array(arr)\n",
    "      \n",
    "    # calculate the range between 2 bounds\n",
    "    # scale the array into 0-1 bounds\n",
    "    # scale the array into given bounds\n",
    "    rng = bounds[1] - bounds[0]  \n",
    "    arr = (arr - min(arr))/(max(arr) - min(arr))\n",
    "    arr = (arr * rng) + bounds[0]\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "# global variables in order:\n",
    "# the tags(labels) for tweets\n",
    "# the colors for each tag\n",
    "# run the tweets listener in the background\n",
    "# init a streaming plot object\n",
    "# init a empty plotting data source\n",
    "tags = ['nba', 'nfl', 'mlb']                                \n",
    "cols = ['red', 'blue', 'green']                             \n",
    "_    = Popen([\"python\", \"TweetsListener_private.py\"])               \n",
    "plt  = sp.StreamingPlot(interval=5, width=900, height=900)  \n",
    "pds  = {'x':[], 'y':[], 'color':[], 'tags':[], 'size':[], 'alpha':[]}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Receive and clean streaming tweets:\n",
    "\n",
    "Run the listerner.py file as a background app, the incoming tweets will be available at IP:172.17.0.2, port:5555. (this IP address is the docker container rather than local machine's IP).\n",
    "\n",
    "On the Spark end, the stream is directed into a data abstraction called DStream which is a sequence of RDD's. Given interval time, each RDD holds tweets came in through socket during last interval and are distributed on different machines in cluster. And when we apply operations on the DStream, we are actually manipulating the RDD underlying.\n",
    "\n",
    "Because the in coming stream is messy, there are situations when multiple tweets in one line or one tweet in multiple line. When we receive tweets, we first organize them by delimiter so each row in RDD will be a single tweet.\n",
    "\n",
    "    example raw tweets:\n",
    "\tHere's every Tom Brady Postseason TD! #tbt #NFLPlayoffs https://t.co/2CIHBpz2OW...\n",
    "\tRT @ChargersRHenne: This guy seems like a class act.  I will root for him\n",
    "\tRT @NBA: Kyrie ready! #Celtics #NBALondon https://t.co/KgZVsREGUK...\n",
    "\tRT @NBA: The Second @NBAAllStar Voting Returns! https://t.co/urTwnGQNKl...\n",
    "\t...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up spark configuration using local mode with all CPU's\n",
    "# create a spark context with this configuration\n",
    "# create a streaming context and set the interval to 5 second.\n",
    "# set log level for spark context\n",
    "# set checkpoint directory for spark streaming context\n",
    "conf = SparkConf().setMaster(\"local[*]\")\n",
    "sc   = SparkContext(conf=conf)\n",
    "ssc  = StreamingContext(sc, batchDuration=5)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# create a DStream by listening to socket\n",
    "# For each RDD in stream:\n",
    "# - split the tweets by delimiter \"><\"\n",
    "# - flat map all the rows contains list\n",
    "# get n rows: (tweet)\n",
    "rawTweets = ssc.socketTextStream('172.17.0.2',5555)\\\n",
    "    .map(lambda x: x.split(\"><\"))\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    \n",
    "def logRawTweets(rdd):\n",
    "    \"\"\" Save the first 5 raw tweets in external text file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(timeHeader())\n",
    "        file.write(\"rawTweet:\\n\\t{} raw tweets received from port:\\n\".format(rdd.count()))\n",
    "        for row in rdd.take(5): \n",
    "            file.write('\\t' + str(row[:80]) + '...\\n')\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        # calculate how much time spent on this step\n",
    "        t_1 = datetime.now()\n",
    "        file.write(\"time for receiving raw tweets: \"+str(t_1 - t_0)+\"\\n\")\n",
    "\n",
    "rawTweets.foreachRDD(logRawTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the raw stream, each tweet is a line of string consist of alphabets, numbers, punctuation marks and possibly symbols. In this preprocessing step, we will split the string into a list of clean words, remove symbols like punctuation marks and emojis and assign a tag for each tweet.\n",
    "\n",
    "    example cleaned tweets after preprocessing:\n",
    "\ttag:1, words:['rt', 'chargersrhenne', 'this', 'guy', ...],\n",
    "\ttag:0, words:['rt', 'debruynekev', 'amp', 'ilkayguendogan', ...],\n",
    "\ttag:0, words:['rt', 'commissioner', 'adam', 'silver', ...],\n",
    "\ttag:0, words:['rt', 'spurs', 'all', 'star', ...],\n",
    "\ttag:0, words:['nbaallstar', 'karlanthony', 'towns', 'nbavote', ...],\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignTag(tweet):\n",
    "    \"\"\" Assign a tag for given tweet and remove the tag word from this tweet. \n",
    "    If a tweet has none of the tag words, it is discarded. If a tweet has\n",
    "    more than one tag words, it is assign a tag in order: NBA, NFL, MLB.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag in tweet:\n",
    "            return (i, [word for word in tweet if word != tag])\n",
    "\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - split the words\n",
    "# - strip the symbols and keep only numbers and alphabets\n",
    "# - lower case the words\n",
    "# - remove empty strings\n",
    "# - assign tags for each tweet and remove tag words\n",
    "# get n rows: (tag, [word_0, word_1, ..., word_m])\n",
    "cleanTweets = rawTweets\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .map(lambda xs: [sub('[^A-Za-z0-9]+', '', x) for x in xs])\\\n",
    "    .map(lambda xs: [x.lower() for x in xs])\\\n",
    "    .map(lambda xs: [x for x in xs if x != ''])\\\n",
    "    .map(assignTag)\\\n",
    "    .filter(lambda x: x != None)\n",
    "\n",
    "    \n",
    "def logCleanTweets(rdd):\n",
    "    \"\"\" Save first 5 cleaned tweets and time spent in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\ncleanTweet:\\n\\tcleaned tweets after preprocessing:\\n\")\n",
    "        for row in rdd.take(5):\n",
    "            w = str(row[1][:4]).rstrip(']') + ', ...],\\n'\n",
    "            file.write('\\ttag:'+str(row[0])+\", words:\" + w)\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for cleaning tweets: \"+str(datetime.now()-t_1)+\"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "        \n",
    "rddPrint =  lambda x: x.print(n=5)\n",
    "    \n",
    "cleanTweets.foreachRDD(rddPrint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split training and testing data set from clean tweets stream. One third of the tweets are preserved for future result validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each RDD in stream:\n",
    "    # n rows: (tag, [word_0, word_1, ..., word_m])\n",
    "    # - zip with index: (index, (tag, [word_0, word_1, ..., word_m]))\n",
    "    # - mod the index by 3: (index%3, (tag, [word_0, word_1, ..., word_m]))\n",
    "    cleanTweets_mod = cleanTweets\\\n",
    "        .transform(lambda rdd: rdd.zipWithIndex())\\\n",
    "        .map(lambda x: (x[0] % 3, x[1]))\n",
    "\n",
    "    # For each RDD in stream:\n",
    "    # n rows: (index%3, (tag, [word_0, word_1, ..., word_m]))\n",
    "    # - filter the rows with remainder equals to 0 or 1\n",
    "    # - take the label and list of words: (tag, [word_0, word_1, ..., word_m])\n",
    "    cleanTweets_train = cleanTweets_mod\\\n",
    "        .filter(lambda x: x[0] == 0 or 1)\\\n",
    "        .map(lambda x: x[1])\n",
    "\n",
    "    # For each RDD in stream:\n",
    "    # n rows: (index%3, (tag, [word_0, word_1, ..., word_m]))\n",
    "    # - filter the rows with remainder equals to 2\n",
    "    # - take the label and list of words: (tag, [word_0, word_1, ..., word_m])\n",
    "    cleanTweets_test = cleanTweets_mod\\\n",
    "        .filter(lambda x: x[0] == 2)\\\n",
    "        .map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature extraction: \n",
    "Next, we count the frequency of each word ever appeared in the streaming tweets and keep updating the counts when new tweets coming in.\n",
    "\n",
    "    wordCount:\n",
    "\t('rt', 196)\n",
    "\t('the', 174)\n",
    "\t('in', 85)\n",
    "\t('for', 62)\n",
    "\t('to', 59)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWordCount(newValues, oldCount):\n",
    "    \"\"\" Update the word count with new data.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    newValues: (word, 1) tuples in this round\n",
    "    oldCount: (word, count) tuples upto last round.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    (word, count) tuples upto this round.\n",
    "    \"\"\"\n",
    "    return sum(newValues) + (oldCount or 0)\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - remove the tag\n",
    "# - flatten the list of words so each row has one word\n",
    "# - add 1 count to each word\n",
    "# - use the new word count to update old word count incrementally\n",
    "# - sort the word and count tuple by the count in descending order\n",
    "# get n rows: (word, count)\n",
    "wordCount = cleanTweets\\\n",
    "    .map(lambda x: x[1])\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .updateStateByKey(updateWordCount)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the word counts, we take the 5000 most frequent words as features for tweets dataset. Each feature value in dataset is either True or False indicating whether corresponding tweet contains this feature word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each RDD in stream:\n",
    "# - zip rows with index\n",
    "# - filter the first 5000 rows\n",
    "# - discard the index\n",
    "# get 5000 rows: (word, count)\n",
    "features = wordCount\\\n",
    "    .transform(lambda rdd: rdd.zipWithIndex())\\\n",
    "    .filter(lambda x: x[1] < 5000)\\\n",
    "    .map(lambda x: x[0])\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - take only the word\n",
    "# get 5000 rows: (word)\n",
    "words = features.map(lambda x: x[0])\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - take only the count\n",
    "# get 5000 rows: (count)\n",
    "counts = features.map(lambda x: x[1])\n",
    "\n",
    "\n",
    "def logFeatureWords(rdd):\n",
    "    \"\"\" Save first 5 word count and time spent in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nfeatures:\\n\\twords used as features and their counts:\\n\")\n",
    "        for row in rdd.take(num=5): file.write('\\t' + str(row) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for generating and extracting features:\" \\\n",
    "                   + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "features.foreachRDD(logFeatureWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Feature predictiveness learning\n",
    "\n",
    "### encode dataset\n",
    "With the feature words avaliable, we can transform the tweets into a structured dataset for further analysis. Because the amount of tweets in 5 seconds is limited. Here we use window slicing to retrieve and combine tweets in a longer range of time. To be specific, we will fetch all tweets in last 15 seconds and combine them as one RDD every 5 second.\n",
    "\n",
    "\texample encoded dataset:\n",
    "\ttag: 0, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 1, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 2, features: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
    "\ttag: 0, features: [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...],\n",
    "\ttag: 1, features: [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, ...],\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeDataSet(rdd_a, rdd_b):\n",
    "    \"\"\" Encode given tweets by given feature words.\n",
    "    If a feature word appears in this tweet, the corresponding\n",
    "    feature is 1, otherwise 0.\n",
    "    \"\"\"\n",
    "    words_  = rdd_b.collect()\n",
    "    rdd_new = rdd_a.mapValues(lambda x: \\\n",
    "        ([int(word in x) for word in words_], 1))\n",
    "    return rdd_new\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - get tweets in 15s window every 5s\n",
    "# - encoded the tweets using features and append value 1\n",
    "#   onto each tweet to count the number of tweets later.\n",
    "# get rows: (tag, ([1, 0, ..., 1], 1))\n",
    "dataset_train = cleanTweets_train\\\n",
    "    .transformWith(encodeDataSet, words)\n",
    "    \n",
    "    \n",
    "def logDataset(rdd):\n",
    "    \"\"\" Save first 10 rows in encoded dataset and time spent \n",
    "    in this step in log file.\"\"\"\n",
    "    global t_1\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nDataset:\\n\\tencoded dataset:\\n\")\n",
    "        for row in rdd.take(10):\n",
    "            t = str(row[0])\n",
    "            f = str(row[1][0][:10]).rstrip(']') + ', ...],\\n'\n",
    "            file.write(\"\\ttag: \" + t + \", features: \" + f)\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for encoding dataset:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "dataset.foreachRDD(logDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the conditional probability and \"predictiveness\" of features\n",
    "\n",
    "**p(feature|tag)**:   \n",
    "Since all the feature values are either 0 or 1, we can easily get the probability of one feature conditioned on any tag by summing up the values of this column under corresponding tag and divide it by the total count of tweets under this tag.\n",
    "\n",
    "**predictiveness**:  \n",
    "Then we calculate the predictiveness of each feature from this dataset. The predictiveness of one feature quantifies how well can a feature discriminate the label of a tweet from other 2 labels.\n",
    "\n",
    "    1. If we have only 2 labels, it can be defined by the bigger ratio of conditional probabilities of this feature given tags.\n",
    "    \n",
    "$$pdtn(label_1, label_2) = arg max\\left( \\frac{p(word|label_1)}{p(word|label_2)}, \\frac{p(word|label_2)}{p(word|label_1)}\\right)$$\n",
    "\n",
    "\\*Note that this measure is symmetric. In other words, how much a feature can distinguish label 1 from label 2 is the same as how much it can distinguish label 2 from label 1.\n",
    "    \n",
    "    2. When we have more than 2 labels, we can take the average of the predictivenesses of all label combinations.\n",
    "    \n",
    "$$pdtn(label_1, label_2, label_3) = \\frac{pdtn(label_1, label_2) + pdtn(label_1, label_3) + pdtn(label_2, label_3)}{3}$$\n",
    "\n",
    "    3. At last this predictiveness of feature word should be weighted by the count of this word. The more frequent this word appears, the more reliable this predictiveness is.\n",
    "\n",
    "$$pdtn = pdtn \\times count$$\n",
    "\n",
    "    most predictive words:\n",
    "\tword        : (cp0, cp1, cp2, pdtn)\n",
    "\tdon         : (0.15934065934065933, 0.0078125, 0.05555555555555555, 283.4985066666667)\n",
    "\tallstar     : (0.14835164835164835, 0.0078125, 0.05555555555555555, 249.3439)\n",
    "\tthe         : (0.46153846153846156, 0.4765625, 0.3333333333333333, 244.9167533333333)\n",
    "\talabama     : (0.005494505494505495, 0.140625, 0.05555555555555555, 216.67129)\n",
    "\trt          : (0.5879120879120879, 0.625, 0.5555555555555556, 211.0108)\n",
    "\tfitzpatrick : (0.005494505494505495, 0.1328125, 0.05555555555555555, 195.5925333333333)\n",
    "\tvoting      : (0.12637362637362637, 0.0078125, 0.05555555555555555, 187.45217333333335)\n",
    "\tminkah      : (0.005494505494505495, 0.125, 0.05555555555555555, 175.55554999999998)\n",
    "\tdraft       : (0.016483516483516484, 0.171875, 0.1111111111111111, 149.7176)\n",
    "\tknow        : (0.16483516483516483, 0.0234375, 0.05555555555555555, 131.95061333333334)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elementWiseAdd(list_1, list_2):\n",
    "    \"\"\" Add up 2 lists element-wisely\"\"\"\n",
    "    return [a + b for a, b in zip(list_1, list_2)]\n",
    "\n",
    "def calcPDTN(cps):\n",
    "    \"\"\" takes in the conditional probablities of one feature \n",
    "    and calculate the predictiveness of this feature.\n",
    "    \"\"\"\n",
    "    cp0, cp1, cp2 = tuple(cps)\n",
    "    f    = lambda a, b: round(max(a/b, b/a), 5)\n",
    "    pdtn = (f(cp0, cp1) + f(cp0, cp2) + f(cp1, cp2))/3\n",
    "\n",
    "    return (cp0, cp1, cp2, pdtn)\n",
    "\n",
    "\n",
    "def weightByCount(rdd_a, rdd_b):\n",
    "    \"\"\" Weight the predictiveness of features by their counts.\"\"\"\n",
    "    # collect the counts as a list\n",
    "    # for each row in rdd:\n",
    "    # weight the pdtn by corresponding count\n",
    "    # get 5000 rows: (index, ((PDTN_m, count_m), weightedPDTN_m))\n",
    "    cnts    = rdd_b.collect()\n",
    "    rdd_new = rdd_a.map(lambda x: (x[0], (x[1][3]*cnts[x[0]], cnts[x[0]])))\n",
    "    \n",
    "    return rdd_new\n",
    "        \n",
    "    \n",
    "def zipWithWords(rdd_a, rdd_b):\n",
    "    \"\"\" Replace the feature index with word.\n",
    "    \"\"\"\n",
    "    words_ = rdd_b.collect()\n",
    "    return rdd_a.map(lambda x: (words_[x[0]], x[1]))\n",
    "\n",
    "\n",
    "# For each RDD in stream:\n",
    "# - reduce the RDD by key so that each RDD has only two rows,\n",
    "#   each row has 3 elements: the tag, counts of appearances \n",
    "#   of each feature word in the tweets and the count of tweets\n",
    "# get 3 rows corresponding to 3 tags: \n",
    "#   (tag, ([featCnt_0, featCnt_1, ..., featCnt_4999], tweetCnt))\n",
    "# - divide the count of each feature word by the count of tweets\n",
    "#   to get the conditional probability. We add 1 to the count \n",
    "#   to avoid zero divide error.\n",
    "# get 3 rows: (tag, [cp_0, cp_1, ..., cp_4999])\n",
    "# - for each row, combine each cp with feature index\n",
    "# get 3 rows: ((0, cp_0), (1, cp_1), ..., (4999, cp_4999))\n",
    "# - flatmap these 3 rows\n",
    "# get 15000 rows: (m, cp_m)\n",
    "# - reduce by key(index) to combine cps for different tags\n",
    "# get 5000 rows: (index, (cp0_m, cp1_m, cp2_m))\n",
    "# - for each row, calculate predictiveness using conditional prob\n",
    "# get 5000 rows: (index, (cp0_m, cp1_m, cp2_m, pdtn_m))\n",
    "# - weight the pdtn by the count of the words\n",
    "# get 5000 rows: (index_m, (weightedPDTN_m, count_m))\n",
    "# - replace the word index by the actual word\n",
    "# get 5000 rows: (word_m, (weightedPDTN_m, count_m))\n",
    "featureStats = dataset_train\\\n",
    "    .reduceByKey(lambda a, b: (elementWiseAdd(a[0], b[0]), a[1]+b[1]))\\\n",
    "    .mapValues(lambda x: [(count+1)/x[1] for count in x[0]])\\\n",
    "    .map(lambda x: [(i, cp) for i, cp in enumerate(x[1])])\\\n",
    "    .flatMap(lambda x: x)\\\n",
    "    .mapValues(lambda x: [x])\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .map(lambda x: (x[0], calcPDTN(x[1])))\\\n",
    "    .transformWith(weightByCount, counts)\\\n",
    "    .transformWith(zipWithWords, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update feature predictiveness dictionary\n",
    "We are keeping a dictionary of feature words and their corresponding predictiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWordStats(newValue, oldValue):\n",
    "    \"\"\" Update the word stats with new data.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    newValue: (weightedPDTN_m, count_m) tuples in this round\n",
    "    oldValue: (weightedPDTN_m, count_m) tuples up to last round.\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    (PDTN_new, count_m) tuples upto this round.\n",
    "\n",
    "    \"\"\"\n",
    "    oldValue = oldValue or (0, 0)\n",
    "    count_new = oldValue[1] + newValue[1]\n",
    "    PDTN_new = (oldValue[0] * oldValue[1] + newValue[0] * newValue[1]) / count_new\n",
    "    \n",
    "    return (PDTN_new, count_new)\n",
    "   \n",
    "# For each RDD in stream:\n",
    "# - merge with the old state and generate new state\n",
    "# get n rows: (word, (weightedPDTN_m, count_m))\n",
    "wordStats = featureStats\\\n",
    "    .updateStateByKey(updateWordStats)\\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: -x[1][0]))\n",
    "\n",
    "def logWordStats(rdd):\n",
    "    \"\"\" Save top 10 most predictive words and their stats to log file.\"\"\"\n",
    "    global t_1\n",
    "    rdd_new = rdd.sortBy(lambda x: -x[1][3])\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nmost predictive words:\\n\\tword : (cp0, cp1, cp2, pdtn)\\n\")\n",
    "        for row in rdd_new.take(10): \n",
    "            file.write(\"\\t\" + row[0] + \" : \" + str(row[1]) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for calculating predictiveness:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "\n",
    "wordStats.foreachRDD(logWordStats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Validate the learned features on testing tweets\n",
    "To validate whether this feature predictiveness makes sense, we will visualize testing tweets based on the learned most predictive features.\n",
    "\n",
    "### Encode testing tweets with 300 most predictive features\n",
    "\n",
    "Under each label, 60 tweets are selected from the testing tweets. These 180 tweets are encoded as structured dataset using 300 most predictive features selected out of 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchFeatsTweets(rdd_a, rdd_b):\n",
    "    \"\"\" Given predictiveness of each feature word, fetch 300 most\n",
    "    predictive features and 180 most predictable tweets under 3 tags\n",
    "    from encoded dataset.\n",
    "    \"\"\"\n",
    "    # For each RDD:\n",
    "    # - take the word and pdtn of features\n",
    "    # get 5000 rows: (word, pdtn_i)\n",
    "    # - sort by pdtn\n",
    "    # get 5000 rows: (word, pdtn_i)\n",
    "    # - take the first 300 rows\n",
    "    # get 300 rows: (word, pdtn_i)\n",
    "    predFeats = rdd_b\\\n",
    "        .map(lambda x: (x[0], x[1][3]))\\\n",
    "        .sortBy(lambda x: -x[1])\\\n",
    "        .take(num=300)\n",
    "\n",
    "    words = predFeats.map(lambda x: x[0]).collect()\n",
    "    pdtns = predFeats.map(lambda x: x[1]).collect()\n",
    "\n",
    "    sumPDTN = lambda a, b: sum([i*k for i, k in zip(a, b)])\n",
    "\n",
    "    # for each row in rdd:\n",
    "    # has n rows: (tag, ([feat_0, ..., feat_4999], 1))\n",
    "    # - keeps only high predictive features\n",
    "    # get n rows: (tag, [feat_0, ..., feat_299])\n",
    "    # - calculate the sum pdtn\n",
    "    # get n rows: (tag, [feat_0, ..., feat_299], sumPDTN)\n",
    "    # - take the first 60 rows:\n",
    "    # get 60 rows: (tag, [feat_0, ..., feat_299], sumPDTN)\n",
    "    rdd_new = rdd_a\\\n",
    "        .map(lambda x: (x[0], [x[1][0][i] for i in indices]))\\\n",
    "        .map(lambda x: (x[0], x[1], sumPDTN(x[1], pdtns)))\\\n",
    "        .take(num=60)\n",
    "\n",
    "    return rdd_new\n",
    "\n",
    "# for each tag, filter dataset DStream with corresponding tag\n",
    "# fetch high predictive features and predictable tweets.\n",
    "ds_0 = cleanTweets_test.filter(lambda x: x[0] == 0)\\\n",
    "    .transformWith(fetchFeatsTweets, wordStats)\n",
    "ds_1 = cleanTweets_test.filter(lambda x: x[0] == 1)\\\n",
    "    .transformWith(fetchFeatsTweets, wordStats)\n",
    "ds_2 = cleanTweets_test.filter(lambda x: x[0] == 2)\\\n",
    "    .transformWith(fetchFeatsTweets, wordStats)\n",
    "\n",
    "# concatenate 3 DStreams together\n",
    "# get 300 rows: (tag, [feat_0, ..., feat_99], sumPDTN)\n",
    "selectedTweets = ds_0.union(ds_1).union(ds_2)\n",
    "\n",
    "def logSelectedTweets(rdd):\n",
    "    \"\"\" Save first 5 selected tweets and features to log file.\"\"\"\n",
    "    global t_1\n",
    "    cnt = rdd.count()\n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nselected {} tweets and 100 features for plotting:\\n\".format(cnt))\n",
    "        for row in rdd.take(5): \n",
    "            feats = str(row[1][:20]).rstrip(']') + \", ...],\"\n",
    "            file.write(\"\\t\" + str(row[0]) + \", \" + feats + str(row[2]) + \"\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for filtering tweets and features:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "    \n",
    "selectedTweets.foreachRDD(logSelectedTweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension deduction and visualization\n",
    "Since the dataset we have are sparse and have binary values, here we use non-linear method t-SNE to learn the 2-d manifold the dataset lies on to obtain x values and y values for our each tweet. The tweets are then visualized as scatter plot. In the scatter, each circle represents a tweet. The color is identified by the tag-NBA tweets will be red, NFL tweets will be blue and MLB ones will be green. The size and alpha of circle is identified by the sum of predictivenesses of each tweet. And the x, y values are 2 dimensions coming out of t-SNE. If the circles of same color are easily distinguishable from the other colors, then the features are effective for classifying this tag.\n",
    "\n",
    "\\*All the numeric attributes are standardized using min-max scaler to produce more readable result.\n",
    "\n",
    "    plotting data source:\n",
    "\tTotal number of tweets:138\n",
    "\tNumber of NBA tweets; NFL tweets; MLB tweets : [60, 60, 18]\n",
    "\tx     : [ 0.17929186  0.18399966  0.63295108  0.17661807...]\n",
    "\ty     : [ 0.62392987  0.66881742  0.69876889  0.36454208...]\n",
    "\tcolor : ['red', 'red', 'red', 'red'...]\n",
    "\ttags  : ['nba', 'nba', 'nba', 'nba'...]\n",
    "\tsize  : [0.0042378419396584864, 0.0042378419396584864, 0.0042378419396584864, 0.0042378419396584864...]\n",
    "\talpha : [ 0.42378419  0.42378419  0.42378419  0.42378419...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectPDS(rdd):\n",
    "    \"\"\" collect the tag, feature values and predictiveness sum \n",
    "    and fit-transform the features with TSNE.\n",
    "    \"\"\"\n",
    "    global tsne_0, tsne_1, color, label, alpha, t_1\n",
    "    \n",
    "    # transform tags indices to colors, and tags\n",
    "    # collect data to master machine\n",
    "    pData = rdd.map(lambda x: (cols[x[0]], tags[x[0]], x[2], x[1]))\n",
    "    pData = pData.collect()\n",
    "    \n",
    "    # reduce the dimensionality to 2 using t-SNE\n",
    "    # scale the learned features to range [0, 1]\n",
    "    array  = np.array([x[3] for x in pData])\n",
    "    if len(array.shape) == 1: \n",
    "        array = array.reshape(1, -1)\n",
    "    embed  = TSNE(n_components=2).fit_transform(array)\n",
    "    tsne_0 = minMaxScale(embed[:, 0])\n",
    "    tsne_1 = minMaxScale(embed[:, 1])\n",
    "    \n",
    "    color = [x[0] for x in pData]\n",
    "    label = [x[1] for x in pData]\n",
    "    alpha = [x[2] for x in pData]\n",
    "    \n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\ntime for TSNE learning:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "    t_1 = datetime.now()\n",
    "    \n",
    "selectedTweets.foreachRDD(collectPDS)\n",
    "\n",
    "\n",
    "def printPDS(pds):\n",
    "    \"\"\" Save plotting data source to log file.\"\"\"\n",
    "    global t_1\n",
    "    lens = [len([x for x in pds['color'] if x == i]) for i in cols]\n",
    "    \n",
    "    with open(\"logs.txt\", 'a') as file:\n",
    "        file.write(\"\\nplotting data source:\\n\")\n",
    "        file.write(\"\\tTotal number of tweets:\" + str(len(pds['x'])) + \"\\n\")\n",
    "        file.write(\"\\tNumber of NBA tweets; NFL tweets; MLB tweets : \" + str(lens) + \"\\n\")\n",
    "        for item in pds.items(): \n",
    "            file.write(\"\\t\" + item[0] + \" : \" + str(item[1][:4]).rstrip(']') + \"...]\\n\")\n",
    "        file.write(\"...\\n\")\n",
    "        \n",
    "        file.write(\"time for collecting plotting data source:\" + str(datetime.now()-t_1) + \"\\n\")\n",
    "        t_1 = datetime.now()\n",
    "        file.write(\"\\nTiming for this round: {}\\n\".format(t_1 - t_0))\n",
    "\n",
    "        \n",
    "def updatePDS(rdd):\n",
    "    \"\"\" update the dictionary containing data for plotting.\n",
    "    \"\"\"\n",
    "    global alpha\n",
    "    alpha = minMaxScale(alpha, (0.2, 0.8))\n",
    "    sizes = [i/100 for i in alpha]\n",
    "    \n",
    "    global pds\n",
    "    pds['x']     = tsne_0\n",
    "    pds['y']     = tsne_1\n",
    "    pds['color'] = color\n",
    "    pds['tags']  = label\n",
    "    pds['alpha'] = alpha\n",
    "    pds['size']  = sizes\n",
    "    \n",
    "    printPDS(pds)\n",
    "     \n",
    "selectedTweets.foreachRDD(updatePDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computation pipeline set, we start the Spark Streaming context and show the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "plt.start(pds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
